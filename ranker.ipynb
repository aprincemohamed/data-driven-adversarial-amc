{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "import os,random\n",
    "import csv\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, AlphaDropout, GlobalAveragePooling1D, BatchNormalization, add,LSTM,GRU,Bidirectional, Attention\n",
    "from tensorflow.python.keras.layers.core import Flatten, Dense, Dropout, Reshape, Activation,Lambda,Permute\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D,Conv2D, AveragePooling1D, ZeroPadding2D, Convolution2D, MaxPooling1D, MaxPooling2D\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle,time\n",
    "import random, sys\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1260000, 2, 128)\n"
     ]
    }
   ],
   "source": [
    "Xd = pickle.load(open('/media/ext_storage/ahmed_backup/ML/RML22-main/code/RML22_org.pickle', 'rb'))\n",
    "snrs,mods = map(lambda j: sorted( list( set( map( lambda x: x[j], Xd.keys() ) ) ) ), [1,0])\n",
    "\n",
    "X = []\n",
    "Y_snr = []\n",
    "lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        X.append(Xd[(mod,snr)])\n",
    "        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))\n",
    "    Y_snr = Y_snr + [mod]*120000\n",
    "X = np.vstack(X)\n",
    "clip_value = max(abs(float(np.min(X))), float(np.max(X)))\n",
    "print(X.shape)\n",
    "Y_snr = np.vstack(Y_snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(yin):\n",
    "    yy = list(yin)\n",
    "    yy1 = np.zeros([len(yy), max(yy)+1])\n",
    "    yy1[np.arange(len(yy)),yy] = 1\n",
    "    return yy1\n",
    "Y_snr = to_onehot(map(lambda x: mods.index(lbl[x][0]), range(X.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network input shape in variable input-shp: [2, 128]\n",
      "Network output shape in variable input-shp: 10\n",
      "(630000, 2, 128) [2, 128]\n"
     ]
    }
   ],
   "source": [
    "# Partition the dataset into training and testing datasets\n",
    "np.random.seed(2016)\n",
    "n_examples = X.shape[0]\n",
    "n_train    = int(round(n_examples * 0.5))\n",
    "train_idx  = np.random.choice(range(0,n_examples), size=n_train, replace=False)\n",
    "test_idx   = list(set(range(0,n_examples))-set(train_idx))\n",
    "X_train    = X[train_idx]\n",
    "X_test     = X[test_idx]\n",
    "\n",
    "Y_train = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), train_idx)))\n",
    "Y_test = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), test_idx)))\n",
    "\n",
    "in_shp = list(X_train.shape[1:])\n",
    "output_shp = Y_train.shape[1]\n",
    "print(f'Network input shape in variable input-shp: {in_shp}')\n",
    "print(f'Network output shape in variable input-shp: {output_shp}')\n",
    "print(X_train.shape, in_shp)\n",
    "classes = mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig = deepcopy(X_train)\n",
    "X_test_orig = deepcopy(X_test)\n",
    "\n",
    "num_samples_list = [16, 32, 64]\n",
    "field_names= ['num_subsample'] + list(snrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker_types = [\"complex_cnn\", \"resnet\", \"cldnn\"]\n",
    "\n",
    "for ranker_type in ranker_types:\n",
    "    if ranker_type == \"resnet\":\n",
    "        # build model ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        def residual_stack(x):\n",
    "            def residual_unit1(y,_strides=1):\n",
    "                shortcut_unit=y\n",
    "                # 1x1 conv linear\n",
    "                y = layers.Conv1D(8, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "\n",
    "                y = layers.Conv1D(8, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                # add batch normalization\n",
    "                y = layers.add([shortcut_unit,y])\n",
    "                return y\n",
    "    \n",
    "            def residual_unit2(y,_strides=1):\n",
    "                shortcut_unit=y\n",
    "                # 1x1 conv linear\n",
    "                y = layers.Conv1D(16, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                y = layers.Conv1D(16, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                          kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                # add batch normalization\n",
    "                y = layers.add([shortcut_unit,y])\n",
    "                return y\n",
    "    \n",
    "            def residual_unit3(y,_strides=1):\n",
    "                shortcut_unit=y\n",
    "                # 1x1 conv linear\n",
    "                y = layers.Conv1D(32, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                y = layers.Conv1D(32, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                # add batch normalization\n",
    "                y = layers.add([shortcut_unit,y])\n",
    "                return y\n",
    "    \n",
    "            def residual_unit4(y,_strides=1):\n",
    "                shortcut_unit=y\n",
    "                # 1x1 conv linear\n",
    "                y = layers.Conv1D(64, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                y = layers.Conv1D(64, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                # add batch normalization\n",
    "                y = layers.add([shortcut_unit,y])\n",
    "                return y\n",
    "    \n",
    "            x = layers.Conv1D(8, data_format='channels_first',kernel_size=1, padding='same',activation='relu',\n",
    "                      kernel_initializer='glorot_uniform')(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "\n",
    "            x = residual_unit1(x)\n",
    "            x = layers.Conv1D(16, data_format='channels_first',kernel_size=1, padding='same',activation='relu',\n",
    "                     kernel_initializer='glorot_uniform')(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = residual_unit2(x)\n",
    "            x = layers.Conv1D(32, data_format='channels_first',kernel_size=1, padding='same',activation='relu',\n",
    "                     kernel_initializer='glorot_uniform')(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = residual_unit3(x)\n",
    "            x = layers.Conv1D(64, data_format='channels_first',kernel_size=1, padding='same',activation='relu',\n",
    "                     kernel_initializer='glorot_uniform')(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = residual_unit4(x)\n",
    "            x = layers.AveragePooling1D(data_format='channels_first',pool_size=in_shp[1])(x)\n",
    "            return x\n",
    "    \n",
    "        inputs=layers.Input(shape=in_shp)\n",
    "        x = residual_stack(inputs)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256,kernel_initializer=\"he_normal\", activation=\"selu\", name=\"dense1\")(x)\n",
    "        x = AlphaDropout(0.5)(x)\n",
    "        x = Dense(len(classes),kernel_initializer=\"he_normal\", activation=\"softmax\", name=\"dense3\")(x)\n",
    "        model = models.Model(inputs,x)\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "    elif ranker_type == \"cldnn\":\n",
    "        dr = 0.5 # dropout rate (%)\n",
    "        model = models.Sequential()\n",
    "        model.add(Reshape([1] + input_shp, input_shape=input_shp))\n",
    "        model.add(ZeroPadding2D((0, 2), data_format=\"channels_first\"))\n",
    "        model.add(Conv2D(kernel_initializer=\"glorot_uniform\", name=\"conv1\", activation=\"relu\", data_format=\"channels_first\", padding=\"valid\", filters=256, kernel_size=(1, 3)))\n",
    "        model.add(Dropout(dr))\n",
    "\n",
    "        model.add(ZeroPadding2D((0, 2), data_format=\"channels_first\"))\n",
    "        model.add(Conv2D(kernel_initializer=\"glorot_uniform\", name=\"conv2\", activation=\"relu\", data_format=\"channels_first\", padding=\"valid\", filters=256, kernel_size=(2, 3)))\n",
    "        model.add(Dropout(dr))\n",
    "\n",
    "        model.add(ZeroPadding2D((0, 2), data_format=\"channels_first\"))\n",
    "        model.add(Conv2D(kernel_initializer=\"glorot_uniform\", name=\"conv3\", activation=\"relu\", data_format=\"channels_first\", padding=\"valid\", filters=80, kernel_size=(1, 3)))\n",
    "        model.add(Dropout(dr))\n",
    "\n",
    "        model.add(ZeroPadding2D((0, 2), data_format=\"channels_first\"))\n",
    "        model.add(Conv2D(kernel_initializer=\"glorot_uniform\", name=\"conv4\", activation=\"relu\", data_format=\"channels_first\", padding=\"valid\", filters=80, kernel_size=(1, 3)))\n",
    "        model.add(Dropout(dr))\n",
    "        model.add(ZeroPadding2D((0, 2), data_format=\"channels_first\"))\n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.add(Reshape((1,11200)))\n",
    "        model.add(LSTM(50))\n",
    "        model.add(Dense(256, activation='relu', kernel_initializer='he_normal', name=\"dense1\"))\n",
    "        model.add(Dropout(dr))\n",
    "        model.add(Dense( len(classes), kernel_initializer='he_normal', name=\"dense2\" ))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.add(Reshape([len(classes)]))\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        \n",
    "    else:        \n",
    "        dr=0.5\n",
    "        model = Sequential()\n",
    "        model.add(Reshape(input_shp+[1], input_shape=input_shp))\n",
    "        model.add(ZeroPadding2D((1, 2)))\n",
    "        model.add(Convolution2D(256, (2, 3), padding='valid', activation='relu', name=\"conv1\", kernel_initializer='glorot_uniform'))#ch from 3->4\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Permute((3,2,1)))\n",
    "        model.add(Lambda(LC))\n",
    "        model.add(Permute((3,2,1)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(dr))\n",
    "        model.add(Convolution2D(80, (2, 3), padding='valid', activation=\"relu\", name=\"conv2\", kernel_initializer='glorot_uniform'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dr))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu', kernel_initializer='he_normal', name=\"dense1\"))\n",
    "        model.add(Dropout(dr))\n",
    "        model.add(Dense( len(classes), kernel_initializer='he_normal', name=\"dense2\" ))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.add(Reshape([len(classes)]))\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        \n",
    "        \n",
    "    # Number of epochs\n",
    "    epochs = 200\n",
    "    # Training batch size\n",
    "    batch_size = 512\n",
    "    \n",
    "    #train Complex\n",
    "    start = time.time()\n",
    "    #filepath = 'complex.wts.h5'\n",
    "    filepath = 'ranker_' + ranker_type + '.wts.h5'\n",
    "\n",
    "    history = model.fit(X_train,Y_train,batch_size=batch_size,epochs=epochs,verbose=2,validation_split = 0.2,\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto'),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15, verbose=1, mode='auto')\n",
    "        ])\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print('Training time = ' + str(round(duration/60,5)) + 'minutes')\n",
    "    \n",
    "    \n",
    "    field_names= ['num_subsample'] + list(snrs)\n",
    "    acc = {}\n",
    "    acc[\"num_subsample\"] = 128\n",
    "    for snr in snrs:\n",
    "        # extract classes @ SNR\n",
    "        print(snr)\n",
    "        test_SNRs = list(map(lambda x: lbl[x][1], test_idx))\n",
    "        test_X_i = X_test[np.where(np.array(test_SNRs)==snr)]\n",
    "        test_Y_i = Y_test[np.where(np.array(test_SNRs)==snr)]\n",
    "\n",
    "        test_Y_i_hat = model.predict(test_X_i, batch_size=batch_size)\n",
    "        conf = np.zeros([len(classes),len(classes)])\n",
    "        confnorm = np.zeros([len(classes),len(classes)])\n",
    "        for i in range(0,test_X_i.shape[0]):\n",
    "            j = list(test_Y_i[i,:]).index(1)\n",
    "            k = int(np.argmax(test_Y_i_hat[i,:]))\n",
    "            conf[j,k] = conf[j,k] + 1\n",
    "        for i in range(0,len(classes)):\n",
    "            confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])  \n",
    "        cor = np.sum(np.diag(conf))\n",
    "        ncor = np.sum(conf) - cor\n",
    "        print (\"Overall Accuracy: \", cor / (cor+ncor))\n",
    "        acc[snr] = 1.0*cor/(cor+ncor)\n",
    "\n",
    "    num_samples = 128\n",
    "    # Save results to a pickle file for plotting later\n",
    "    print (acc)\n",
    "    with open('./results/acc_' + ranker_type + '_Ranker_' + str(num_samples) + '.csv', 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "        writer.writeheader()\n",
    "        writer.writerow(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
