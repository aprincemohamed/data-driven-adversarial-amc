{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "import os,random\n",
    "import csv\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, AlphaDropout, GlobalAveragePooling1D, BatchNormalization, add\n",
    "from tensorflow.python.keras.layers.core import Flatten, Dense, Dropout, Reshape, Activation,Lambda,Permute\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D,Conv2D, AveragePooling1D, ZeroPadding2D, Convolution2D, MaxPooling1D, MaxPooling2D\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle,time\n",
    "import random, sys\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "from art.attacks.evasion import CarliniL2Method, CarliniLInfMethod\n",
    "from art.estimators.classification import KerasClassifier\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1260000, 2, 128)\n"
     ]
    }
   ],
   "source": [
    "# Dataset setup\n",
    "Xd = pickle.load(open('RML22.pickle', 'rb'))\n",
    "\n",
    "snrs,mods = map(lambda j: sorted( list( set( map( lambda x: x[j], Xd.keys() ) ) ) ), [1,0])\n",
    "\n",
    "X = []\n",
    "Y_snr = []\n",
    "lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        X.append(Xd[(mod,snr)])\n",
    "        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))\n",
    "    Y_snr = Y_snr + [mod]*120000\n",
    "X = np.vstack(X)\n",
    "clip_value = max(abs(float(np.min(X))), float(np.max(X)))\n",
    "print(X.shape)\n",
    "Y_snr = np.vstack(Y_snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(yin):\n",
    "    yy = list(yin)\n",
    "    yy1 = np.zeros([len(yy), max(yy)+1])\n",
    "    yy1[np.arange(len(yy)),yy] = 1\n",
    "    return yy1\n",
    "Y_snr = to_onehot(map(lambda x: mods.index(lbl[x][0]), range(X.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network input shape in variable input-shp: [2, 128]\n",
      "Network output shape in variable input-shp: 10\n",
      "(630000, 2, 128) [2, 128]\n"
     ]
    }
   ],
   "source": [
    "# Partition the dataset into training and testing datasets\n",
    "np.random.seed(2022)\n",
    "n_examples = X.shape[0]\n",
    "n_train    = int(round(n_examples * 0.5))\n",
    "train_idx  = np.random.choice(range(0,n_examples), size=n_train, replace=False)\n",
    "test_idx   = list(set(range(0,n_examples))-set(train_idx))\n",
    "X_train    = X[train_idx]\n",
    "X_test     = X[test_idx]\n",
    "\n",
    "Y_train = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), train_idx)))\n",
    "Y_test = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), test_idx)))\n",
    "\n",
    "input_shp = list(X_train.shape[1:])\n",
    "output_shp = Y_train.shape[1]\n",
    "print(f'Network input shape in variable input-shp: {input_shp}')\n",
    "print(f'Network output shape in variable input-shp: {output_shp}')\n",
    "print(X_train.shape, input_shp)\n",
    "classes = mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig = deepcopy(X_train)\n",
    "X_test_orig = deepcopy(X_test)\n",
    "\n",
    "num_samples_list = [16, 32, 64]\n",
    "field_names= ['num_subsample'] + list(snrs)\n",
    "acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subSampling_schemes = [\"complex_cnn\", \"resnet\", \"cldnn\", \"holistic\", \"uniform\"]\n",
    "\n",
    "for subSampling_scheme in subSampling_schemes:\n",
    "\n",
    "    for num_samples in num_samples_list:\n",
    "\n",
    "        # Data-driven subsampling\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        X_train = deepcopy(X_train_orig)\n",
    "        X_test =  deepcopy(X_test_orig)\n",
    "\n",
    "\n",
    "        if subSampling_scheme == \"holistic\":\n",
    "            cldnn = load_model('ranker_cldnn.wts.h5')\n",
    "            cnn = load_model('ranker_complex_cnn.wts.h5')\n",
    "            resnet = load_model('ranker_resnet.wts.h5')  \n",
    "        elif subSampling_scheme != \"uniform\":\n",
    "            orig_model = load_model('ranker_' + subSampling_scheme + '.wts.h5')\n",
    "\n",
    "        X_train_sub = np.zeros((X_train.shape[0], X_train.shape[1], num_samples))\n",
    "        X_test_sub = np.zeros((X_test.shape[0], X_test.shape[1], num_samples))\n",
    "        print(X_train_sub.shape)\n",
    "\n",
    "        for i, snr in enumerate(snrs):\n",
    "            print('SNR = ', snr)\n",
    "            #train data\n",
    "            train_SNRs = list(map(lambda x: lbl[x][1], train_idx))\n",
    "            orig_idxs = np.where(np.array(train_SNRs) == snr)[0]\n",
    "            snr_data = X_train[orig_idxs]\n",
    "            snr_out = Y_train[orig_idxs]\n",
    "            snr_acc_list = []\n",
    "            snr_data_copy = deepcopy(snr_data)\n",
    "            # print(f\"Number of train data with SNR = {snr} is {orig_idxs.shape}\")\n",
    "\n",
    "            #test Data\n",
    "            test_SNRs = list(map(lambda x: lbl[x][1], test_idx))\n",
    "            orig_idxs_test = np.where(np.array(test_SNRs)== snr)[0]\n",
    "            test_data =  X_test[orig_idxs_test]\n",
    "            # print(f\"Number of test data with SNR = {snr} is {orig_idxs_test.shape}\")\n",
    "\n",
    "            if subSampling_scheme == \"holistic\":\n",
    "                #~~~~~~~~~~~~~~~~Holistic~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                holistic_list = []\n",
    "                try:\n",
    "                    with open('./data/resnet_index/indexList_' + str(snr)+ '.data', 'rb') as fp:\n",
    "                        resnet_list = pickle.load(fp)\n",
    "                    with open('./data/complex_cnn_index/indexList_' + str(snr)+ '.data', 'rb') as fp:\n",
    "                        cnn_list = pickle.load(fp)\n",
    "                    with open('./data/cldnn_index/indexList_' + str(snr)+ '.data', 'rb') as fp:\n",
    "                        cldnn_list = pickle.load(fp)\n",
    "\n",
    "                except:\n",
    "                    cldnn_list = []\n",
    "                    cnn_list = []\n",
    "                    resnet_list = []\n",
    "                    snr_data_copy = deepcopy(snr_data)\n",
    "                    for idx in range(X.shape[2]):\n",
    "                        snr_data = deepcopy(snr_data_copy)\n",
    "                        snr_data = snr_data.transpose((2, 1, 0))\n",
    "                        new_snr_data = np.append(snr_data[:idx], np.zeros((1, snr_data.shape[1], snr_data.shape[2])), axis=0)\n",
    "                        snr_data = np.append(new_snr_data, snr_data[idx+1:], axis=0)\n",
    "                        snr_data = snr_data.transpose((2, 1, 0))\n",
    "\n",
    "                        cldnn_score = cldnn.evaluate(snr_data, snr_out, batch_size=2048, verbose=0)\n",
    "                        cldnn_list.append((idx, cldnn_score[1]))\n",
    "                        cnn_score = cnn.evaluate(snr_data, snr_out, batch_size=2048, verbose=0)\n",
    "                        cnn_list.append((idx, cnn_score[1]))\n",
    "                        resnet_score = resnet.evaluate(snr_data, snr_out, batch_size=2048, verbose=0)\n",
    "                        resnet_list.append((idx, resnet_score[1]))\n",
    "\n",
    "                    with open('./data/resnet_index/indexList_' + str(snr)+ '.data', 'wb') as fp:\n",
    "                        pickle.dump(resnet_list, fp)\n",
    "                    with open('./data/complex_cnn_index/indexList_' + str(snr)+ '.data', 'wb') as fp:\n",
    "                        pickle.dump(cnn_list, fp)\n",
    "                    with open('./data/cldnn_index/indexList_' + str(snr)+ '.data', 'wb') as fp:\n",
    "                        pickle.dump(cldnn_list, fp)\n",
    "\n",
    "                # tier 3\n",
    "                cldnn_list.sort(key=lambda x: x[1])\n",
    "                cldnn_list = cldnn_list[:num_samples]\n",
    "                cldnn_dict = {k: v for k, v in cldnn_list}\n",
    "\n",
    "                cnn_list.sort(key=lambda x: x[1])\n",
    "                cnn_list = cnn_list[:num_samples]\n",
    "                cnn_dict = {k: v for k, v in cnn_list}\n",
    "\n",
    "                resnet_list.sort(key=lambda x: x[1])\n",
    "                resnet_list = resnet_list[:num_samples]\n",
    "                resnet_dict = {k: v for k, v in resnet_list}\n",
    "\n",
    "                # tier 2\n",
    "                cldnn_cnn_list = [(k, cldnn_dict[k] + cnn_dict[k]) for k in set(cldnn_dict).intersection(cnn_dict)]\n",
    "                cldnn_cnn_dict = {k: v for k, v in cldnn_cnn_list}\n",
    "                cnn_resnet_list = [(k, cnn_dict[k] + resnet_dict[k]) for k in set(cnn_dict).intersection(resnet_dict)]\n",
    "                cnn_resnet_dict = {k: v for k, v in cnn_resnet_list}\n",
    "                cldnn_resnet_list = [(k, cldnn_dict[k] + resnet_dict[k]) for k in set(cldnn_dict).intersection(resnet_dict)]\n",
    "                cldnn_resnet_dict = {k: v for k, v in cldnn_resnet_list}\n",
    "\n",
    "                # tier 1\n",
    "                cldnn_cnn_resnet_list = [(k, cldnn_cnn_dict[k] + resnet_dict[k]) for k in set(cldnn_cnn_dict).intersection(resnet_dict)]\n",
    "                cldnn_cnn_resnet_list.sort(key=lambda x: x[1])\n",
    "\n",
    "                tier_1_samples = cldnn_cnn_resnet_list\n",
    "                tier_1 = [ele[0] for ele in tier_1_samples]\n",
    "\n",
    "\n",
    "                # remove tier 1 entries from tier 2\n",
    "                for ele in tier_1:\n",
    "                    del cldnn_cnn_dict[ele]\n",
    "                    del cnn_resnet_dict[ele]\n",
    "                    del cldnn_resnet_dict[ele]\n",
    "\n",
    "                cldnn_cnn_list = [(k, v) for k, v in cldnn_cnn_dict.items()]\n",
    "                cnn_resnet_list = [(k, v) for k, v in cnn_resnet_dict.items()]\n",
    "                cldnn_resnet_list = [(k, v) for k, v in cldnn_resnet_dict.items()]\n",
    "\n",
    "                tier_2_samples = cldnn_cnn_list + cnn_resnet_list + cldnn_resnet_list\n",
    "                tier_2_samples.sort(key=lambda x: x[1])\n",
    "\n",
    "                tier_2 = [ele[0] for ele in tier_2_samples]\n",
    "\n",
    "\n",
    "                # remove tier 1 & tier 2 entries from tier 3\n",
    "                for ele in tier_1:\n",
    "                    del cldnn_dict[ele]\n",
    "                    del cnn_dict[ele]\n",
    "                    del resnet_dict[ele]\n",
    "\n",
    "                for ele in tier_2:\n",
    "                    try: del cldnn_dict[ele]\n",
    "                    except: pass\n",
    "                    try: del cnn_dict[ele]\n",
    "                    except: pass\n",
    "                    try: del resnet_dict[ele]\n",
    "                    except: pass\n",
    "\n",
    "                cldnn_list = [(k, v) for k, v in cldnn_dict.items()]\n",
    "                cnn_list = [(k, v) for k, v in cnn_dict.items()]\n",
    "                resnet_list = [(k, v) for k, v in resnet_dict.items()]\n",
    "\n",
    "                tier_3_samples = cldnn_list + cnn_list + resnet_list\n",
    "                tier_3_samples.sort(key=lambda x: x[1])\n",
    "\n",
    "                holistic_list = holistic_list + tier_1_samples\n",
    "                holistic_list = holistic_list + tier_2_samples\n",
    "                holistic_list = holistic_list + tier_3_samples\n",
    "                holistic_list = holistic_list[:num_samples]\n",
    "                holistic_list.sort(key=lambda x: x[1])\n",
    "                holistic_list.sort(key=lambda x: x[0])\n",
    "\n",
    "                snr_idxs = [ele[0] for ele in holistic_list]\n",
    "                #~~~~~~~~~~~~~~~~Holistic~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "            elif subSampling_scheme == \"uniform\":\n",
    "                #~~~~~~~~~~~~~~~~Uniform~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                snr_idxs = list(range(1, 128, int(128/num_samples)))\n",
    "                #~~~~~~~~~~~~~~~~Uniform~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "            else: \n",
    "                #~~~~~~~~~~~~~~~~ResNet/CNN/CLDNN~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                try:\n",
    "                    with open('./data/' + subSampling_scheme + '_index/indexList_' + str(snr)+ '.data', 'rb') as fp:\n",
    "                        snr_acc_list = pickle.load(fp)\n",
    "                except:\n",
    "                    # only using train data for subsampling\n",
    "                    for idx in range(X_train.shape[2]):\n",
    "                        snr_data = deepcopy(snr_data_copy)\n",
    "                        snr_data = snr_data.transpose((2,1,0))\n",
    "                        new_snr_data = np.append(snr_data[:idx], np.full((1, snr_data.shape[1], snr_data.shape[2]), np.mean(snr_data[idx])), axis=0)\n",
    "                        snr_data = np.append(new_snr_data, snr_data[idx+1:], axis=0)\n",
    "                        snr_data = snr_data.transpose((2,1,0))\n",
    "                        score = orig_model.evaluate(snr_data, snr_out, batch_size=len(orig_idxs), verbose=0)\n",
    "                        snr_acc_list.append((idx, score))\n",
    "\n",
    "                    with open('./data/' + subSampling_scheme + '_index/indexList_' + str(snr)+ '.data', 'wb') as fp:\n",
    "                        pickle.dump(snr_acc_list, fp)\n",
    "\n",
    "                snr_acc_list.sort(key=lambda x: x[1])\n",
    "                snr_acc_list = snr_acc_list[:num_samples]\n",
    "                snr_acc_list.sort(key=lambda x: x[0]) \n",
    "                snr_idxs = [ele[0] for ele in snr_acc_list]\n",
    "                #~~~~~~~~~~~~~~~~ResNet/CNN/CLDNN~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "            #train data\n",
    "            snr_data = snr_data.transpose((2,1,0))\n",
    "            snr_data = snr_data[snr_idxs]\n",
    "            snr_data = snr_data.transpose((2,1,0))\n",
    "\n",
    "            #test data\n",
    "            test_data = test_data.transpose((2,1,0))\n",
    "            test_data = test_data[snr_idxs]\n",
    "            test_data = test_data.transpose((2,1,0))\n",
    "\n",
    "            X_train_sub[orig_idxs] = snr_data\n",
    "            X_test_sub[orig_idxs_test] = test_data\n",
    "\n",
    "        X_train = X_train_sub\n",
    "        X_test = X_test_sub\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "        print(X_train.shape)\n",
    "        print(Y_train.shape)\n",
    "\n",
    "        in_shp = list(X_train.shape[1:])\n",
    "        print(X_train.shape, in_shp, snrs)\n",
    "        classes = mods\n",
    "\n",
    "        # build model ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        def residual_stack(x):\n",
    "            def residual_unit1(y,_strides=1):\n",
    "                shortcut_unit=y\n",
    "                # 1x1 conv linear\n",
    "                y = layers.Conv1D(8, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "\n",
    "                y = layers.Conv1D(8, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                # add batch normalization\n",
    "                y = layers.add([shortcut_unit,y])\n",
    "                return y\n",
    "    \n",
    "            def residual_unit2(y,_strides=1):\n",
    "                shortcut_unit=y\n",
    "                # 1x1 conv linear\n",
    "                y = layers.Conv1D(16, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                y = layers.Conv1D(16, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                          kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                # add batch normalization\n",
    "                y = layers.add([shortcut_unit,y])\n",
    "                return y\n",
    "    \n",
    "            def residual_unit3(y,_strides=1):\n",
    "                shortcut_unit=y\n",
    "                # 1x1 conv linear\n",
    "                y = layers.Conv1D(32, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                y = layers.Conv1D(32, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                # add batch normalization\n",
    "                y = layers.add([shortcut_unit,y])\n",
    "                return y\n",
    "    \n",
    "            def residual_unit4(y,_strides=1):\n",
    "                shortcut_unit=y\n",
    "                # 1x1 conv linear\n",
    "                y = layers.Conv1D(64, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                y = layers.Conv1D(64, kernel_size=7,data_format='channels_first',strides=_strides,padding='same',activation='relu',\n",
    "                         kernel_initializer='glorot_uniform')(y)\n",
    "                y = layers.BatchNormalization()(y)\n",
    "                # add batch normalization\n",
    "                y = layers.add([shortcut_unit,y])\n",
    "                return y\n",
    "    \n",
    "            x = layers.Conv1D(8, data_format='channels_first',kernel_size=1, padding='same',activation='relu',\n",
    "                      kernel_initializer='glorot_uniform')(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "\n",
    "            x = residual_unit1(x)\n",
    "            x = layers.Conv1D(16, data_format='channels_first',kernel_size=1, padding='same',activation='relu',\n",
    "                     kernel_initializer='glorot_uniform')(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = residual_unit2(x)\n",
    "            x = layers.Conv1D(32, data_format='channels_first',kernel_size=1, padding='same',activation='relu',\n",
    "                     kernel_initializer='glorot_uniform')(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = residual_unit3(x)\n",
    "            x = layers.Conv1D(64, data_format='channels_first',kernel_size=1, padding='same',activation='relu',\n",
    "                     kernel_initializer='glorot_uniform')(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = residual_unit4(x)\n",
    "            x = layers.AveragePooling1D(data_format='channels_first',pool_size=in_shp[1])(x)\n",
    "            return x\n",
    "    \n",
    "        inputs=layers.Input(shape=in_shp)\n",
    "        x = residual_stack(inputs)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256,kernel_initializer=\"he_normal\", activation=\"selu\", name=\"dense1\")(x)\n",
    "        x = AlphaDropout(0.5)(x)\n",
    "        x = Dense(len(classes),kernel_initializer=\"he_normal\", activation=\"softmax\", name=\"dense3\")(x)\n",
    "        model = models.Model(inputs,x)\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "        nb_epoch = 200     # number of epochs to train on\n",
    "        batch_size = 512  # training batch size\n",
    "\n",
    "        # Train the Model\n",
    "        filepath = './data/model/subsample_' + subSampling_scheme + '_' + str(num_samples) + '.wts.h5'\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                    optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "        history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=2, validation_split=0.2, \n",
    "                callbacks = [\n",
    "                    keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='auto')\n",
    "                    ,keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15, verbose=0, mode='auto')\n",
    "                ])\n",
    "        model.load_weights(filepath)\n",
    "\n",
    "        # Evaluate and Plot Model Performance\n",
    "        score = model.evaluate(X_test, Y_test, verbose=0, batch_size=batch_size)\n",
    "        print(score)\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "        # Evaluate classification accuracy per SNR and store it.\n",
    "        field_names= ['num_subsample'] + list(snrs)\n",
    "        acc = {}\n",
    "        acc[\"num_subsample\"] = num_samples\n",
    "        for snr in snrs:\n",
    "            # extract classes @ SNR\n",
    "            print(snr)\n",
    "            test_SNRs = list(map(lambda x: lbl[x][1], test_idx))\n",
    "            test_X_i = X_test[np.where(np.array(test_SNRs)==snr)]\n",
    "            test_Y_i = Y_test[np.where(np.array(test_SNRs)==snr)]\n",
    "\n",
    "            test_Y_i_hat = model.predict(test_X_i)\n",
    "            conf = np.zeros([len(classes),len(classes)])\n",
    "            confnorm = np.zeros([len(classes),len(classes)])\n",
    "            for i in range(0,test_X_i.shape[0]):\n",
    "                j = list(test_Y_i[i,:]).index(1)\n",
    "                k = int(np.argmax(test_Y_i_hat[i,:]))\n",
    "                conf[j,k] = conf[j,k] + 1\n",
    "            for i in range(0,len(classes)):\n",
    "                confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])  \n",
    "            cor = np.sum(np.diag(conf))\n",
    "            ncor = np.sum(conf) - cor\n",
    "            print (\"Overall Accuracy: \", cor / (cor+ncor))\n",
    "            acc[snr] = 1.0*cor/(cor+ncor)\n",
    "\n",
    "        print (acc)\n",
    "        with open('./results/acc_subsampling_' + subSampling_scheme + '' + str(num_samples) + '.csv', 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "            writer.writeheader()\n",
    "            writer.writerow(acc)\n",
    "        acc_list.append(acc)\n",
    "\n",
    "    with open('./results/acc_subsampling_' + subSampling_scheme + '_all.csv', 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "        writer.writeheader()\n",
    "        for acc_i in acc_list:\n",
    "            writer.writerow(acc_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
